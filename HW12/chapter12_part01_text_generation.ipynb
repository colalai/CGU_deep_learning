{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/colalai/CGU_deep_learning/blob/main/chapter12_part01_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNaP3jWaLvm5"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsQyLQfLLvm7"
      },
      "source": [
        "# Generative deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ttcv1J0xLvm8"
      },
      "source": [
        "## Text generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMevY_0CLvm8"
      },
      "source": [
        "### A brief history of generative deep learning for sequence generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cEb8kxULvm8"
      },
      "source": [
        "### How do you generate sequence data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0F_iaMaLvm8"
      },
      "source": [
        "### The importance of the sampling strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N36341XjLvm8"
      },
      "source": [
        "**Reweighting a probability distribution to a different temperature**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "utV3sSaJLvm8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def reweight_distribution(original_distribution, temperature=0.5):\n",
        "    distribution = np.log(original_distribution) / temperature\n",
        "    distribution = np.exp(distribution)\n",
        "    return distribution / np.sum(distribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w_tNJ-aLvm9"
      },
      "source": [
        "### Implementing text generation with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI2ad3K_Lvm9"
      },
      "source": [
        "#### Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o23V6X8wLvm9"
      },
      "source": [
        "**Downloading and uncompressing the IMDB movie reviews dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Gi96nNfPLvm9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1421724a-6a12-457d-d1f2-8844d7295677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-12 16:34:01--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  17.4MB/s    in 8.4s    \n",
            "\n",
            "2022-12-12 16:34:10 (9.58 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeaNNj-BLvm-"
      },
      "source": [
        "**Creating a dataset from text files (one file = one sample)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sVrsuEm6Lvm-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcd5ed02-156d-406f-d6f4-267a43fc93b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100006 files belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "dataset = keras.utils.text_dataset_from_directory(\n",
        "    directory=\"aclImdb\", label_mode=None, batch_size=256)\n",
        "dataset = dataset.map(lambda x: tf.strings.regex_replace(x, \"<br />\", \" \"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv_ehPtKLvm-"
      },
      "source": [
        "**Preparing a `TextVectorization` layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1Fra5caMLvm-"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "sequence_length = 100\n",
        "vocab_size = 15000\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "text_vectorization.adapt(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2Wu7hgLLvm-"
      },
      "source": [
        "**Setting up a language modeling dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nlF4UypBLvm-"
      },
      "outputs": [],
      "source": [
        "def prepare_lm_dataset(text_batch):\n",
        "    vectorized_sequences = text_vectorization(text_batch)\n",
        "    x = vectorized_sequences[:, :-1]\n",
        "    y = vectorized_sequences[:, 1:]\n",
        "    return x, y\n",
        "\n",
        "lm_dataset = dataset.map(prepare_lm_dataset, num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7aL5WMILvm-"
      },
      "source": [
        "#### A Transformer-based sequence-to-sequence model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "A462RBJuLvm-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "          num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "          num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(TransformerDecoder, self).get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10DlMfPlLvm_"
      },
      "source": [
        "**A simple Transformer-based language model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ky05ONa1Lvm_"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 2\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, x)\n",
        "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q78oComDLvm_"
      },
      "source": [
        "### A text-generation callback with variable-temperature sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z63LYHVZLvm_"
      },
      "source": [
        "**The text-generation callback**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pirimYKwLvnA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "tokens_index = dict(enumerate(text_vectorization.get_vocabulary()))\n",
        "\n",
        "def sample_next(predictions, temperature=1.0):\n",
        "    predictions = np.asarray(predictions).astype(\"float64\")\n",
        "    predictions = np.log(predictions) / temperature\n",
        "    exp_preds = np.exp(predictions)\n",
        "    predictions = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, predictions, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "class TextGenerator(keras.callbacks.Callback):\n",
        "    def __init__(self,\n",
        "                 prompt,\n",
        "                 generate_length,\n",
        "                 model_input_length,\n",
        "                 temperatures=(1.,),\n",
        "                 print_freq=1):\n",
        "        self.prompt = prompt\n",
        "        self.generate_length = generate_length\n",
        "        self.model_input_length = model_input_length\n",
        "        self.temperatures = temperatures\n",
        "        self.print_freq = print_freq\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if (epoch + 1) % self.print_freq != 0:\n",
        "            return\n",
        "        for temperature in self.temperatures:\n",
        "            print(\"== Generating with temperature\", temperature)\n",
        "            sentence = self.prompt\n",
        "            for i in range(self.generate_length):\n",
        "                tokenized_sentence = text_vectorization([sentence])\n",
        "                predictions = self.model(tokenized_sentence)\n",
        "                next_token = sample_next(predictions[0, i, :])\n",
        "                sampled_token = tokens_index[next_token]\n",
        "                sentence += \" \" + sampled_token\n",
        "            print(sentence)\n",
        "\n",
        "prompt = \"This movie\"\n",
        "text_gen_callback = TextGenerator(\n",
        "    prompt,\n",
        "    generate_length=50,\n",
        "    model_input_length=sequence_length,\n",
        "    temperatures=(0.2, 0.5, 0.7, 1., 1.5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7pcOS9gLvnA"
      },
      "source": [
        "**Fitting the language model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ndfp1PBlLvnA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39c47330-7efe-406c-bc13-eb77f527f7bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "  6/391 [..............................] - ETA: 2:38 - loss: 5.4415"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1888s vs `on_train_batch_end` time: 0.2194s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "391/391 [==============================] - ETA: 0s - loss: 5.1453== Generating with temperature 0.2\n",
            "This movie film is seems often to [UNK] handle [UNK] silverman everything town seems and to filming include ups from of any men different responsible contrasts sword no and power no totally apparent sum reason it for out just as do an that accident without that [UNK] she into is listening pure\n",
            "== Generating with temperature 0.5\n",
            "This movie film was [UNK] very on good television and when continuous i revive understood her that story it is remains told is in told world by [UNK] bettany hand and the from film a alone dozen how corners it of is a a murder technical of finding an out american as\n",
            "== Generating with temperature 0.7\n",
            "This movie is is why a i comedy feel i i completely personally expressed [UNK] but i basically cant entire believe life who but direct seen to [UNK] find of the sexual beyond [UNK] that that the the film way will to be get able up to to end get since [UNK]\n",
            "== Generating with temperature 1.0\n",
            "This movie was about a one little judges too who [UNK] was and throughout but the he same is off great he if is [UNK] trouble his with past his he girlfriend wanted as here his himself trip he to is his as 1970 academy but his is brother just the a\n",
            "== Generating with temperature 1.5\n",
            "This movie was was pretty very good good but in i the think [UNK] the doesnt film really the had dumb some side wasnt of all when the a real [UNK] person his who [UNK] s and go leave to that italy an it damaging is scenes to are dont easily get\n",
            "391/391 [==============================] - 168s 428ms/step - loss: 5.1453\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.7522== Generating with temperature 0.2\n",
            "This movie show is everything so about good what actors gets are real really underground bad moments for especially children from one this night movie i must thought wonder that how rainbow essential was than destroyed the my shoved whole was long food with i explore had this its movie blatantly around\n",
            "== Generating with temperature 0.5\n",
            "This movie animated is version often of overlooked the amazing [UNK] tragedy [UNK] in 2 1984 films one [UNK] of most the popular films directors i and truly upon want everything a to success do if more you strong get family a and insight trapped i in gave less me than and\n",
            "== Generating with temperature 0.7\n",
            "This movie is has obvious a spoiler little be summer killed skipped by by the reactions [UNK] from [UNK] increasingly family policy home following sucks with the a whole girl clothing played its by based a family young added child to jones that by can the be [UNK] as of a it\n",
            "== Generating with temperature 1.0\n",
            "This movie endurance seemed success the to last me year for of english the her [UNK] prowess to at recreate presenting the high [UNK] quality [UNK] of as this it parody was [UNK] a 1992 [UNK] horror general style but many who details never [UNK] actually so death well a after social\n",
            "== Generating with temperature 1.5\n",
            "This movie movie was parody only of worth sexual a torture price sequences for poor a direction long for hair a was wealth almost in swearing kind loss of and quiet possessions [UNK] treasures along wary with of effective gold [UNK] [UNK] deliciously [UNK] officially [UNK] [UNK] for [UNK] revenge rambo of\n",
            "391/391 [==============================] - 168s 428ms/step - loss: 4.7522\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.5639== Generating with temperature 0.2\n",
            "This movie is got painful utter and [UNK] plain im as aware it of is public very enemies strange fathers and on devices a apparently new not low the budget ability must to take represent it absolute and worst use of of stories special i abuse have myself seen three [UNK] days\n",
            "== Generating with temperature 0.5\n",
            "This movie movie came acting in all realistic poker and movie certainly theater not it against could most with likely the it exception have of been many missing people stories the yet cast infantile of actors their in lives order attempt who to webb reach the a audience companion watch the this\n",
            "== Generating with temperature 0.7\n",
            "This movie movie is starred about leo a mccarey storm named on georgia the [UNK] dating coproduction from of [UNK] venice [UNK] and are financed being by saved a by student grade [UNK] dutch student [UNK] values voodoo instead doll this whose is girlfriend partially date missing nails her numbers exgirlfriend from\n",
            "== Generating with temperature 1.0\n",
            "This movie one wore appears called to sadako be starring lying in unfortunately the she police resembles this that except is when the sleeping woods queen her herself husband convict [UNK] acting her way way than leave a her 60 aunts seconds 31 we years have that in happening money and to\n",
            "== Generating with temperature 1.5\n",
            "This movie movie features looked and great quite it top released it it off backwards but a it camcorder chooses and for who the falls ride in through the the afternoon coma in for the the fourth mexican part project 2 a whale [UNK] budapest that on was a a desert mature\n",
            "391/391 [==============================] - 167s 425ms/step - loss: 4.5639\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.4485== Generating with temperature 0.2\n",
            "This movie has excellent always comments been are on certainly a the read more of the it novel refers its to the the plight same of in [UNK] what shows is such a a question bet ever with it a but true the story atmosphere plays which the is whole spent mans\n",
            "== Generating with temperature 0.5\n",
            "This movie is is wonderful the it characters is are superb so like cast slightly and [UNK] there the part problem as is a that sexy nearly lara so flynn called is cop now who killing they them get on over rotten the walls top the billing other and than it bottles\n",
            "== Generating with temperature 0.7\n",
            "This movie is is so about short everything stories and from being [UNK] presented but in its this atmosphere language 1 short year a old fantastic house [UNK] full and of a energetic more and forbidden idiotic love dialogues story causing lines a of lot leave going you [UNK] talking with to\n",
            "== Generating with temperature 1.0\n",
            "This movie show was which coming shown from on the television tv channel and after again a it one wasnt i remotely thought interested it in was a status convicted as of a a kids angry movie child but both should were be her assassinated first i sons found see the her\n",
            "== Generating with temperature 1.5\n",
            "This movie movie is was a not major as movie good for considering fox the its hype classic and comedy an that entertaining will it loose improve british it farce as coupled a with bad fresh fx and plot [UNK] lines is and slow special and if recycled you footage could this\n",
            "391/391 [==============================] - 168s 429ms/step - loss: 4.4485\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - ETA: 0s - loss: 4.3660== Generating with temperature 0.2\n",
            "This movie [UNK] tells [UNK] really stories some and credit observations it of makes being a made work by of such french modern epics [UNK] this but one it of compares the to previous american 50 graffiti man artist who story penned is [UNK] a and very a lively must script see\n",
            "== Generating with temperature 0.5\n",
            "This movie was is the brilliant greatest neil ever simon there masterpiece ive in seen league also is i amazing agree there however is somewhere no between site the but filmmaker with graham never greene put for the great shame entertainment that sidney it lumet may is or one giving of him\n",
            "== Generating with temperature 0.7\n",
            "This movie movie is was completely filmed predictable in crummy a south acts america white couldnt midget stop as imitating many lightning musical humorous numbers interactions for with the both odd thrilled bacall comedy who scenes delivers vaguely an [UNK] attempt with at foul comedy mouthed i provides found a quite big\n",
            "== Generating with temperature 1.0\n",
            "This movie movie makes deals me 14 like years visitor ago q and a it fascinating must to have see been watched left quite cold an and electric original chair film each about time getting it close this at one the from top when city it lights was up on subtly i\n",
            "== Generating with temperature 1.5\n",
            "This movie movie is was just not like even talented such but luck that in it my is friends a challenge goodnatured and family johnson it was directed saved by from a southern grade audience c by [UNK] two for sons whom and can daughters tell perform you educational words stereotype muscle\n",
            "391/391 [==============================] - 168s 428ms/step - loss: 4.3660\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7b95d98550>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "model.fit(lm_dataset, epochs=5, callbacks=[text_gen_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMx-edtXLvnA"
      },
      "source": [
        "### Wrapping up"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
